{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor index,row in traindf.iterrows():\\n    img = Image.open(\"../Train/\"+row[0])\\n    img = img.resize((pix,pix), Image.ANTIALIAS)\\n    img.save(\"../Train/\"+row[0]) \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "pix = 32\n",
    "traindf = pd.read_csv(\"./train.csv\")\n",
    "\"\"\"\n",
    "Resizing images to 32 x 32 pixels\n",
    "\n",
    "for index,row in traindf.iterrows():\n",
    "    img = Image.open(\"../Train/\"+row[0])\n",
    "    img = img.resize((pix,pix), Image.ANTIALIAS)\n",
    "    img.save(\"../Train/\"+row[0]) \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor index , row in testdf.iterrows():\\n    img = Image.open(\"../Test/\"+row[0])\\n    img = img.resize((pix,pix), Image.ANTIALIAS)\\n    img.save(\"../Test/\"+row[0]) \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf = pd.read_csv(\"./test.csv\")\n",
    "\"\"\"\n",
    "Resizing images to 32 x 32 pixels\n",
    "\n",
    "for index , row in testdf.iterrows():\n",
    "    img = Image.open(\"../Test/\"+row[0])\n",
    "    img = img.resize((pix,pix), Image.ANTIALIAS)\n",
    "    img.save(\"../Test/\"+row[0]) \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from skimage import io, transform\n",
    "from PIL import Image\n",
    "import os\n",
    "agedict = {'YOUNG':1 , 'MIDDLE' :2 , 'OLD':3}\n",
    "class IMFDBDataset(Dataset):\n",
    "    \"\"\"IMFDB dataset\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.actor_image = pd.read_csv(csv_file)\n",
    "        self.imgname = self.actor_image.iloc[:,0]\n",
    "        self.imgage = self.actor_image.iloc[:,1]\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.actor_image.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.root_dir + \"/\" + self.imgname[idx]\n",
    "        image = Image.open(img_name)\n",
    "        age = agedict[self.imgage[idx]]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image,age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "fullset = IMFDBDataset(csv_file='./train.csv',root_dir='../Train',transform = transform)\n",
    "trainset, testset = torch.utils.data.random_split(fullset, [14000, 5906])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=20,shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=20,shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPlotting Random train images\\n\\nimport matplotlib.pyplot as plt\\nfrom torchvision import transforms, utils\\nimport numpy as np\\ndef imshow(img):\\n    img = img/2 + 0.5     # unnormalize\\n    npimg = img.numpy()\\n    x = np.transpose(npimg, (1, 2, 0))\\n    plt.imshow(x)\\n\\ndataiter = iter(trainloader)\\nimages ,label = dataiter.next()\\nimshow(utils.make_grid(images))\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\"\"\"\n",
    "Plotting Random train images\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, utils\n",
    "import numpy as np\n",
    "def imshow(img):\n",
    "    img = img/2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    x = np.transpose(npimg, (1, 2, 0))\n",
    "    plt.imshow(x)\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images ,label = dataiter.next()\n",
    "imshow(utils.make_grid(images))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 1.261\n",
      "[1,   400] loss: 0.930\n",
      "[1,   600] loss: 0.898\n",
      "[2,   200] loss: 0.807\n",
      "[2,   400] loss: 0.782\n",
      "[2,   600] loss: 0.776\n",
      "[3,   200] loss: 0.729\n",
      "[3,   400] loss: 0.737\n",
      "[3,   600] loss: 0.737\n",
      "[4,   200] loss: 0.707\n",
      "[4,   400] loss: 0.689\n",
      "[4,   600] loss: 0.687\n",
      "[5,   200] loss: 0.665\n",
      "[5,   400] loss: 0.665\n",
      "[5,   600] loss: 0.657\n",
      "[6,   200] loss: 0.619\n",
      "[6,   400] loss: 0.640\n",
      "[6,   600] loss: 0.624\n",
      "[7,   200] loss: 0.599\n",
      "[7,   400] loss: 0.610\n",
      "[7,   600] loss: 0.596\n",
      "[8,   200] loss: 0.573\n",
      "[8,   400] loss: 0.561\n",
      "[8,   600] loss: 0.579\n",
      "[9,   200] loss: 0.545\n",
      "[9,   400] loss: 0.538\n",
      "[9,   600] loss: 0.534\n",
      "[10,   200] loss: 0.513\n",
      "[10,   400] loss: 0.491\n",
      "[10,   600] loss: 0.512\n",
      "[11,   200] loss: 0.452\n",
      "[11,   400] loss: 0.483\n",
      "[11,   600] loss: 0.466\n",
      "[12,   200] loss: 0.420\n",
      "[12,   400] loss: 0.433\n",
      "[12,   600] loss: 0.438\n",
      "[13,   200] loss: 0.391\n",
      "[13,   400] loss: 0.391\n",
      "[13,   600] loss: 0.416\n",
      "[14,   200] loss: 0.327\n",
      "[14,   400] loss: 0.366\n",
      "[14,   600] loss: 0.348\n",
      "[15,   200] loss: 0.320\n",
      "[15,   400] loss: 0.338\n",
      "[15,   600] loss: 0.318\n",
      "[16,   200] loss: 0.265\n",
      "[16,   400] loss: 0.290\n",
      "[16,   600] loss: 0.307\n",
      "[17,   200] loss: 0.253\n",
      "[17,   400] loss: 0.247\n",
      "[17,   600] loss: 0.257\n",
      "[18,   200] loss: 0.190\n",
      "[18,   400] loss: 0.227\n",
      "[18,   600] loss: 0.216\n",
      "[19,   200] loss: 0.168\n",
      "[19,   400] loss: 0.177\n",
      "[19,   600] loss: 0.191\n",
      "[20,   200] loss: 0.141\n",
      "[20,   400] loss: 0.146\n",
      "[20,   600] loss: 0.164\n",
      "[21,   200] loss: 0.124\n",
      "[21,   400] loss: 0.100\n",
      "[21,   600] loss: 0.135\n",
      "[22,   200] loss: 0.096\n",
      "[22,   400] loss: 0.101\n",
      "[22,   600] loss: 0.113\n",
      "[23,   200] loss: 0.068\n",
      "[23,   400] loss: 0.109\n",
      "[23,   600] loss: 0.105\n",
      "[24,   200] loss: 0.058\n",
      "[24,   400] loss: 0.084\n",
      "[24,   600] loss: 0.086\n",
      "[25,   200] loss: 0.062\n",
      "[25,   400] loss: 0.056\n",
      "[25,   600] loss: 0.069\n",
      "[26,   200] loss: 0.073\n",
      "[26,   400] loss: 0.074\n",
      "[26,   600] loss: 0.079\n",
      "[27,   200] loss: 0.036\n",
      "[27,   400] loss: 0.049\n",
      "[27,   600] loss: 0.088\n",
      "[28,   200] loss: 0.043\n",
      "[28,   400] loss: 0.054\n",
      "[28,   600] loss: 0.085\n",
      "[29,   200] loss: 0.024\n",
      "[29,   400] loss: 0.020\n",
      "[29,   600] loss: 0.057\n",
      "[30,   200] loss: 0.039\n",
      "[30,   400] loss: 0.022\n",
      "[30,   600] loss: 0.035\n",
      "[31,   200] loss: 0.022\n",
      "[31,   400] loss: 0.018\n",
      "[31,   600] loss: 0.024\n",
      "[32,   200] loss: 0.022\n",
      "[32,   400] loss: 0.013\n",
      "[32,   600] loss: 0.007\n",
      "[33,   200] loss: 0.009\n",
      "[33,   400] loss: 0.015\n",
      "[33,   600] loss: 0.008\n",
      "[34,   200] loss: 0.037\n",
      "[34,   400] loss: 0.034\n",
      "[34,   600] loss: 0.042\n",
      "[35,   200] loss: 0.014\n",
      "[35,   400] loss: 0.023\n",
      "[35,   600] loss: 0.013\n",
      "[36,   200] loss: 0.057\n",
      "[36,   400] loss: 0.031\n",
      "[36,   600] loss: 0.023\n",
      "[37,   200] loss: 0.038\n",
      "[37,   400] loss: 0.054\n",
      "[37,   600] loss: 0.046\n",
      "[38,   200] loss: 0.036\n",
      "[38,   400] loss: 0.021\n",
      "[38,   600] loss: 0.042\n",
      "[39,   200] loss: 0.015\n",
      "[39,   400] loss: 0.011\n",
      "[39,   600] loss: 0.014\n",
      "[40,   200] loss: 0.007\n",
      "[40,   400] loss: 0.004\n",
      "[40,   600] loss: 0.006\n",
      "[41,   200] loss: 0.013\n",
      "[41,   400] loss: 0.007\n",
      "[41,   600] loss: 0.006\n",
      "[42,   200] loss: 0.007\n",
      "[42,   400] loss: 0.006\n",
      "[42,   600] loss: 0.004\n",
      "[43,   200] loss: 0.002\n",
      "[43,   400] loss: 0.002\n",
      "[43,   600] loss: 0.001\n",
      "[44,   200] loss: 0.002\n",
      "[44,   400] loss: 0.001\n",
      "[44,   600] loss: 0.002\n",
      "[45,   200] loss: 0.002\n",
      "[45,   400] loss: 0.001\n",
      "[45,   600] loss: 0.000\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class AGE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AGE, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, 3,1,(1,1))\n",
    "        self.conv = nn.Conv2d(10, 30, 3,1,(1,1))\n",
    "        self.batch_0=nn.BatchNorm2d(30)\n",
    "        self.conv2 = nn.Conv2d(30, 60, 3)\n",
    "        self.batch_1=nn.BatchNorm2d(60)\n",
    "        self.fc1 = nn.Linear(7*7*60, 500)\n",
    "        self.batch_2=nn.BatchNorm1d(500)\n",
    "        self.fc2 = nn.Linear(500, 300)\n",
    "        self.batch_3=nn.BatchNorm1d(300)\n",
    "        self.fc3 = nn.Linear(300, 100)\n",
    "        self.fc4 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.batch_0(self.conv(x)))\n",
    "        x = F.relu(self.batch_1(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 7*7*60)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "    def name(self):\n",
    "        return \"AGE\"\n",
    "\n",
    "import torch.optim as optim\n",
    "model = AGE()\n",
    "model.cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum = 0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(45):\n",
    "    # trainning\n",
    "    ave_loss = 0\n",
    "    for i,data in enumerate(trainloader,0):\n",
    "        inputs, label = data\n",
    "        inputs, label = inputs.cuda(), label.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(inputs)\n",
    "        loss = criterion(out,label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ave_loss+= loss.item()\n",
    "        if i % 200 == 199:    # print every 200 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, ave_loss / 200))\n",
    "            ave_loss = 0.0\n",
    "print(\"Finished Training\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5906\n",
      "4709\n",
      "79.73247544869623\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(total)\n",
    "print(correct)\n",
    "print(100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
